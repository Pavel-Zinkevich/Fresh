# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fiK1mrdqH_z8VKc8-yFxpGEKl_f7XlZR
"""

import streamlit as st
from PIL import Image
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Sequential
import zipfile
import os
import tempfile

# -----------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# -----------------------------
IMG_WIDTH, IMG_HEIGHT = 256, 256
VALIDATION_SPLIT = 0.2
SEED = 123
BATCH_SIZE = 32

# -----------------------------
# Data augmentation
# -----------------------------
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
], name="data_augmentation")

# -----------------------------
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
# -----------------------------
def create_cnn_model(num_classes, img_size=(IMG_WIDTH, IMG_HEIGHT, 3), dropout_rate=0.4):
    model = Sequential([
        layers.Input(shape=img_size),
        data_augmentation,
        layers.Rescaling(1./255),

        layers.Conv2D(32, 3, padding='same', activation='relu'),
        layers.MaxPooling2D(2),
        layers.Conv2D(64, 3, padding='same', activation='relu'),
        layers.MaxPooling2D(2),
        layers.Conv2D(128, 3, padding='same', activation='relu'),
        layers.MaxPooling2D(2),
        layers.Dropout(dropout_rate),
        layers.GlobalAveragePooling2D(),

        layers.Dense(64, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

# -----------------------------
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
# -----------------------------
def create_datasets(data_dir):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=VALIDATION_SPLIT,
        subset="training",
        seed=SEED,
        image_size=(IMG_WIDTH, IMG_HEIGHT),
        batch_size=BATCH_SIZE,
        label_mode='categorical'
    )
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=VALIDATION_SPLIT,
        subset="validation",
        seed=SEED,
        image_size=(IMG_WIDTH, IMG_HEIGHT),
        batch_size=BATCH_SIZE,
        label_mode='categorical'
    )
    class_names = train_ds.class_names
    num_classes = len(class_names)
    
    train_ds = train_ds.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)
    val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)
    
    return train_ds, val_ds, class_names, num_classes

# -----------------------------
# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
# -----------------------------
st.title("Fruit Freshness & Type Classifier üçéüçåüçì")

mode = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º", ["Inference", "Train Model"])
epochs = st.sidebar.slider("Epochs", 1, 50, 5)
batch_size = st.sidebar.select_slider("Batch size", [16, 32, 64], value=32)

# -----------------------------
# TRAIN MODE
# -----------------------------
if mode == "Train Model":
    st.header("Train your CNN Model")
    uploaded_zip = st.file_uploader("Upload dataset folder as ZIP", type=["zip"])

    if uploaded_zip is not None:
        with tempfile.TemporaryDirectory() as tmpdir:
            zip_path = os.path.join(tmpdir, "dataset.zip")
            with open(zip_path, "wb") as f:
                f.write(uploaded_zip.getvalue())
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(tmpdir)
            
            data_dir = tmpdir
            st.write("Dataset ready. Creating dataset...")
            train_ds, val_ds, class_names, num_classes = create_datasets(data_dir)
            
            st.write(f"Detected classes: {class_names}")
            model = create_cnn_model(num_classes=num_classes)
            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
            
            if st.button("Start Training"):
                st.write("Training started...")
                history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)
                st.success("Training finished!")
                model.save("trained_model.h5")
                st.write("Model saved as trained_model.h5")

# -----------------------------
# INFERENCE MODE
# -----------------------------
elif mode == "Inference":
    st.header("Inference Mode")
    
    if os.path.exists("trained_model.h5"):
        model = tf.keras.models.load_model("trained_model.h5")
        st.success("Model loaded successfully!")
    else:
        st.warning("No trained model found. Please train the model first.")
    
    uploaded_file = st.file_uploader("Upload image", type=["jpg", "jpeg", "png"])
    
    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("RGB")
        st.image(image, caption="Uploaded Image", use_column_width=True)
        resized_img = image.resize((IMG_WIDTH, IMG_HEIGHT))
        input_array = np.expand_dims(np.array(resized_img)/255.0, axis=0)
        
        pred = model.predict(input_array)
        pred_idx = np.argmax(pred, axis=1)[0]
        st.success(f"Prediction: {pred_idx} ({pred[0][pred_idx]:.2%})")
